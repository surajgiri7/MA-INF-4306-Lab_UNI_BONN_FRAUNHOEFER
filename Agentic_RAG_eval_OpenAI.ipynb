{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    " \n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load OpenAI API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset splits\n",
    "corpus_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\", split=\"passages\")\n",
    "eval_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_dataset[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Convert the text corpus into Document objects\n",
    "documents = [example['passage'] for example in corpus_dataset]\n",
    "\n",
    "documents = [Document(text=text, metadata={\"source\": f\"doc_{i}\"}) for i, text in enumerate(documents)]\n",
    "\n",
    "# Extract the text corpus\n",
    "print(f\"Extracted {len(documents)} passages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize storage and embedding\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  \n",
    "chroma_collection = chroma_client.get_or_create_collection(name=\"knowledge_base\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize OpenAI Embedding model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Embed documents in chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=768, chunk_overlap=56)\n",
    "all_embeddings = []\n",
    "\n",
    "def get_batch_embeddings(batch_chunks):\n",
    "    \"\"\"Fetch embeddings for a batch of chunks.\"\"\"\n",
    "    embeddings = embed_model.embed_documents(batch_chunks)  # Correct method to use after update\n",
    "    return embeddings\n",
    "\n",
    "def process_document(doc, progress_bar):\n",
    "    \"\"\"Process a single document and return its embeddings.\"\"\"\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    batch_size = 100  # Process 100 chunks at once for batching\n",
    "    chunk_batches = [chunks[i:i+batch_size] for i in range(0, len(chunks), batch_size)]\n",
    "    \n",
    "    embeddings_for_doc = []\n",
    "    \n",
    "    # Send batches to be processed concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for batch_embeddings in executor.map(get_batch_embeddings, chunk_batches):\n",
    "            embeddings_for_doc.extend(batch_embeddings)\n",
    "    \n",
    "    # Update the progress bar after processing each document\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    return embeddings_for_doc\n",
    "\n",
    "# Iterate over all documents and process them in parallel with a progress bar\n",
    "def embed_documents(documents):\n",
    "    total_documents = len(documents)\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_documents, desc=\"Processing documents\") as progress_bar:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            embeddings = list(executor.map(process_document, documents, [progress_bar] * total_documents))\n",
    "    \n",
    "    all_embeddings = [embedding for sublist in embeddings for embedding in sublist]\n",
    "    return all_embeddings\n",
    "\n",
    "all_embeddings = embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step: Create FAISS Index\n",
    "embedding_dim = 1536  # Dimension of text-embedding-ada-002\n",
    "\n",
    "# Initialize FAISS index (using L2 distance)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "embedding_matrix = np.array(all_embeddings).astype(np.float32)\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Retrieve Relevant Documents\n",
    "\n",
    "def get_top_k_documents(query, top_k=5):\n",
    "    \"\"\"Retrieve top-k most relevant documents using FAISS.\"\"\"\n",
    "    query_embedding = embed_model.embed_query(query)  # Get query embedding\n",
    "    query_vector = np.array(query_embedding).astype(np.float32).reshape(1, -1)\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "    # Fetch the top-k matching documents\n",
    "    top_docs = [documents[i].text for i in indices[0]]\n",
    "    return top_docs\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What was his first experiment?\"\n",
    "retrieved_docs = get_top_k_documents(query)\n",
    "print(f\"Top documents for query: {query}\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\\n{doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Generate Answer with OpenAI (Updated for latest API)\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    \"\"\"Generate an answer using the query and retrieved documents.\"\"\"\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Create the augmented prompt\n",
    "    prompt = f\"Directly and briefly answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # Use OpenAI's GPT-4 to generate the answer\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    " \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "print(f\"\\nGenerated Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "deepval_prompt = \"\"\"\n",
    "           You are an expert evaluator assessing a Retrieval-Augmented Generation (RAG) system. Your task is to score the system's answer based on the given question and ground truth.\n",
    "\n",
    "            Please assign scores for the following metrics on a scale of 1 to 5. A score of 5 indicates perfect performance, and 1 indicates complete failure. The score should be based on the given instructions.\n",
    "\n",
    "            **Question:** {question}\n",
    "            **Ground Truth Answer:** {ground_truth_answer}\n",
    "            **System Answer:** {retriever_response}\n",
    "\n",
    "            ### Instructions for Scoring:\n",
    "\n",
    "            - **Answer Relevancy (1-5):** How well does the system answer the question? \n",
    "            - A score of 5 means the answer directly addresses the question. \n",
    "            - A score of 1 means the answer is irrelevant or does not address the question at all. \n",
    "            - Scores between 1 and 5 can be given for partial relevance.\n",
    "\n",
    "            - **Faithfulness (1-5):** Is the system answer factually consistent with the information in the retrieved context?\n",
    "            - A score of 5 means the answer is entirely factually correct and consistent with the retrieved content. \n",
    "            - A score of 1 means the answer contains false information.\n",
    "            - Scores between 1 and 5 should reflect degrees of factual correctness.\n",
    "\n",
    "            - **Contextual Precision (1-5):** How specific and concise is the system answer in relation to the ground truth?\n",
    "            - A score of 5 means the system answer is precise and matches the specific details of the ground truth.\n",
    "            - A score of 1 means the system answer is vague or includes incorrect or irrelevant details.\n",
    "\n",
    "            - **Contextual Recall (1-5):** Does the system answer capture the key information from the ground truth?\n",
    "            - A score of 5 means the answer captures all essential details.\n",
    "            - A score of 1 means the answer misses critical information.\n",
    "            - Scores between 1 and 5 reflect partial recall.\n",
    "\n",
    "             Please respond with the scores only in the following format, without any more explanation:\n",
    "            {{\n",
    "                \"answer_relevancy\": <value>,\n",
    "                \"faithfulness\": <value>,\n",
    "                \"contextual_precision\": <value>,\n",
    "                \"contextual_recall\": <value>\n",
    "            }}\n",
    "\n",
    "            If the system answer is perfectly correct, return a score of 5 for all metrics and if it was completely wrong return a score of 1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class QuestionRefinementAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.last_augmented_question = None\n",
    "        self.last_augmented_answer = None  # Store the last augmented answer\n",
    "        self.llm = llm\n",
    "\n",
    "    def is_ambiguous(self, question):\n",
    "        ambiguous_terms = {\"he\", \"him\", \"his\", \"she\", \"her\", \"it\", \"they\", \"them\", \"their\"}\n",
    "        words = set(question.lower().split())\n",
    "        return not words.isdisjoint(ambiguous_terms)\n",
    "\n",
    "    def refine_question(self, current_question):\n",
    "        if self.last_augmented_question and self.is_ambiguous(current_question):\n",
    "            # Use the last augmented answer as context\n",
    "            prompt = f\"Resolve ambiguity in the question by replacing pronouns with their most likely reference and use the following information as context:\\nPrevious Question: {self.last_augmented_question}\\nPrevious Answer: {self.last_augmented_answer}\\nCurrent: {current_question}\\nResolved:\"\n",
    "            \n",
    "            response = self.llm.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        return current_question\n",
    "\n",
    "    def generate_answer_and_store(self, augmented_question, retrieved_docs):\n",
    "        generated_answer = generate_answer(augmented_question, retrieved_docs)\n",
    "        self.last_augmented_answer = generated_answer # store the last answer\n",
    "        return generated_answer\n",
    "\n",
    "\n",
    "def evaluate_agentic_system(eval_dataset, top_k=5):\n",
    "    results = []\n",
    "    question_agent = QuestionRefinementAgent(client)\n",
    "    # eval_dataset = eval_dataset.select(range(35, 45))    \n",
    "    # Iterate directly through the dataset\n",
    "    for example in tqdm(eval_dataset, desc=\"Evaluating system\"):\n",
    "        question = example['question']  \n",
    "        ground_truth_answer = example['answer'] \n",
    "        augmented_question = question_agent.refine_question(question)\n",
    "        retrieved_docs = get_top_k_documents(augmented_question, top_k=top_k)\n",
    "        generated_answer = question_agent.generate_answer_and_store(augmented_question, retrieved_docs) #store answer here\n",
    "        question_agent.last_augmented_question = augmented_question\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": deepval_prompt}]\n",
    "        )\n",
    "\n",
    "        eval_scores_raw = response.choices[0].message.content.strip()\n",
    "        \n",
    "        try:\n",
    "            eval_scores = json.loads(eval_scores_raw)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse scores for question: {question}\")\n",
    "            eval_scores = {\"answer_relevancy\": 1, \"faithfulness\": 1, \"contextual_precision\": 1, \"contextual_recall\": 1}\n",
    "\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"augmented_question\": augmented_question,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"scores\": eval_scores\n",
    "        })  \n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_average_scores(results):\n",
    "    total_scores = {\"answer_relevancy\": 0, \"faithfulness\": 0, \"contextual_precision\": 0, \"contextual_recall\": 0}\n",
    "    num_questions = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        scores = result[\"scores\"]\n",
    "        for metric in total_scores:\n",
    "            total_scores[metric] += scores.get(metric, 0)\n",
    "\n",
    "    avg_scores = {metric: score / num_questions for metric, score in total_scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "def store_results_to_csv(results, filename=\"evaluation_results.csv\"):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"question\", \"augmented_question\", \"answer\", \"generated_answer\", \"answer_relevancy\", \"faithfulness\", \"contextual_precision\", \"contextual_recall\"])\n",
    "        for result in results:\n",
    "            writer.writerow([\n",
    "                result[\"question\"],\n",
    "                result[\"augmented_question\"],\n",
    "                result.get(\"answer\", \"\"), #add this line if answer is available in result\n",
    "                result[\"generated_answer\"],\n",
    "                result[\"scores\"][\"answer_relevancy\"],\n",
    "                result[\"scores\"][\"faithfulness\"],\n",
    "                result[\"scores\"][\"contextual_precision\"],\n",
    "                result[\"scores\"][\"contextual_recall\"]\n",
    "            ])\n",
    "\n",
    "evaluation_results = evaluate_agentic_system(eval_dataset)\n",
    "store_results_to_csv(evaluation_results) #add this line\n",
    "average_scores = calculate_average_scores(evaluation_results)\n",
    "print(\"\\nAverage Scores for Agentic RAG:\")\n",
    "for metric, avg_score in average_scores.items():\n",
    "    print(f\"{metric}: {avg_score:.2f}\")\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
