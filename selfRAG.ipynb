{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai pypdf sentence-transformers chromadb\n",
    "%pip install -qU langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.base import Chain\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key=os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path containing the PDF files\n",
    "folder_path = 'data'\n",
    "\n",
    "# Load all PDF files from the specified directory\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "documents = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(os.path.join(folder_path, pdf_file))\n",
    "    documents.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=768, chunk_overlap=56)\n",
    "split_documents = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create a collection in Chroma\n",
    "collection_name = \"knowledge_base\"\n",
    "chroma_collection = chroma_client.get_or_create_collection(collection_name)\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# Create a Chroma vector store from documents and embeddings\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings_model,\n",
    "    collection_name=\"knowledge_base\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", api_key=api_key)\n",
    "\n",
    "# Function to implement the Self-RAG logic\n",
    "def self_rag(query, retriever, llm, max_iterations=3, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Self-RAG pipeline with iterative refinement.\n",
    "    \"\"\"\n",
    "    context = []\n",
    "    for iteration in range(max_iterations):\n",
    "        # Step 1: Retrieve documents based on the query or refined query\n",
    "        retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "        # Step 2: Generate a response using the retrieved documents\n",
    "        input_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        prompt = (\n",
    "            f\"Context:\\n{input_context}\\n\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            \"Provide a detailed answer based on the context above. If insufficient information is available, specify what additional information is needed.\"\n",
    "        )\n",
    "        response = llm.predict(prompt)\n",
    "\n",
    "        # Step 3: Check if the response is satisfactory\n",
    "        if \"insufficient information\" not in response.lower():\n",
    "            return response  # Exit early if the answer is sufficient\n",
    "\n",
    "        # Step 4: Refine query based on the response\n",
    "        query_refinement_prompt = (\n",
    "            f\"Initial Query: {query}\\n\"\n",
    "            f\"Response: {response}\\n\\n\"\n",
    "            \"What clarifying or follow-up query would help retrieve better context?\"\n",
    "        )\n",
    "        refined_query = llm.predict(query_refinement_prompt).strip()\n",
    "\n",
    "        # Add refined query and responses to context for subsequent iterations\n",
    "        context.append({\"query\": query, \"response\": response})\n",
    "        query = refined_query \n",
    "        \n",
    "\n",
    "    # Return the last response if max iterations are reached\n",
    "    return f\"Final Response (after {max_iterations} iterations): {response}\"\n",
    "\n",
    "query = \"Can you tell me about flow trajectories in detail?\"\n",
    "response = self_rag(query, retriever, llm)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = vector_store.as_retriever()\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\"),\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Can you tell me about flow trajectories in detail?\"\n",
    "# response = qa_chain({\"query\": query})\n",
    "# print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize Conversational Retrieval Chain (RAG with feedback loop) using langchain\n",
    "# qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "#     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "# # Self-assessment logic\n",
    "# def self_rag_query(query):\n",
    "#     # Initial response\n",
    "#     response = qa_chain({\"question\": query, \"chat_history\": []})\n",
    "#     print(\"Initial response:\", response['answer'])\n",
    "\n",
    "#     # Self-reflection\n",
    "#     refinement_query = f\"Based on the above answer, is there any missing or unclear information? If so, reframe the query to improve it. Original query: {query}\"\n",
    "#     reflection = qa_chain({\"question\": refinement_query, \"chat_history\": []})\n",
    "#     print(\"Reflection:\", reflection['answer'])\n",
    "\n",
    "#     # Iterative improvement\n",
    "#     if \"no missing information\" not in reflection['answer'].lower():\n",
    "#         refined_query = reflection['answer']\n",
    "#         improved_response = qa_chain({\"question\": refined_query, \"chat_history\": []})\n",
    "#         return improved_response['answer']\n",
    "#     return response['answer']\n",
    "\n",
    "# # Example usage\n",
    "# query = \"Can you tell me about flow trajectories in detail?\"\n",
    "# final_response = self_rag_query(query)\n",
    "# print(\"Final response:\", final_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
