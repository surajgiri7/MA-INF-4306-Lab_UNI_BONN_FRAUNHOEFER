{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load models once (outside the function)\n",
    "nli_model = pipeline(\"text-classification\", model=\"roberta-large-mnli\", device=0)  # device=0 for GPU\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "DEEPSEEK_TOKEN = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "# Login with Hugging Face token\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Load the model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up the retrieval system using Sentence Transformers\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, documents, embedding_model):\n",
    "        self.documents = [doc.text for doc in documents]\n",
    "        self.embedding_model = embedding_model\n",
    "        self.document_embeddings = self.embed_documents()\n",
    "\n",
    "    def embed_documents(self):\n",
    "        # Encode documents using the Sentence Transformers model\n",
    "        return self.embedding_model.encode(self.documents)\n",
    "\n",
    "    def retrieve(self, query, top_k=10, similarity_threshold=0.3):\n",
    "        # Encode the query\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "\n",
    "        # Compute cosine similarity between query and documents\n",
    "        similarities = np.dot(self.document_embeddings, query_embedding.T).flatten()\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "\n",
    "        # Filter documents based on similarity threshold\n",
    "        relevant_docs = []\n",
    "        for i in top_indices:\n",
    "            if similarities[i] >= similarity_threshold:\n",
    "                relevant_docs.append(self.documents[i])\n",
    "\n",
    "        return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Integrate DeepSeek API for generation with strict document adherence\n",
    "def generate_with_deepseek(query, context):\n",
    "    # Initialize the DeepSeek client\n",
    "    client = OpenAI(\n",
    "        api_key=DEEPSEEK_TOKEN,\n",
    "        base_url=\"https://api.deepseek.com/v1\",\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\", \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant. Answer the following question using only the provided context. Your answer must be concise and no longer than one sentence. If the context does not provide a clear answer, respond with 'Not available.'\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"You are a helpful assistant. Answer the following question using only the provided context. Your answer must be concise and no longer than one sentence. If the context does not provide a clear answer, respond with 'Not available.' Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Combine retrieval and generation\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def run(self, query):\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retriever.retrieve(query)\n",
    "\n",
    "        # If no relevant documents are found, return \"Not available\"\n",
    "        if not retrieved_docs:\n",
    "            return \"Not available or not in context.\"\n",
    "\n",
    "        # Combine retrieved documents into context\n",
    "        context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "        # Generate response using DeepSeek API\n",
    "        response = generate_with_deepseek(query, context)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we use the wikipedia Dataset\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset splits\n",
    "corpus_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\", split=\"passages\")\n",
    "eval_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\", split=\"test\")\n",
    "\n",
    "# Prepare knowledge base from text-corpus\n",
    "corpus_documents = [Document(text=example['passage']) for example in corpus_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corpus Documents:\", corpus_documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document processing with chunking\n",
    "parser = SentenceSplitter(chunk_size=768, chunk_overlap=56)\n",
    "document_chunks = []\n",
    "for doc in corpus_documents:\n",
    "    chunks = parser.split_text(doc.text)\n",
    "    for chunk in chunks:\n",
    "        document_chunks.append(Document(text=chunk))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Initialize components\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "retriever = DocumentRetriever(document_chunks, embedding_model)\n",
    "rag_pipeline = RAGPipeline(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When did he die?\"\n",
    "response = rag_pipeline.run(query)\n",
    "print(\"Generated Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "RAG pipeline is done up to here, now Evaluation\n",
    "---------------------------------\n",
    "---------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Document class to hold text and metadata.\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# --- Retriever Component ---\n",
    "class DocumentRetriever:\n",
    "    def __init__(self, documents, embedding_model):\n",
    "        # Store only the text for retrieval purposes.\n",
    "        self.documents = [doc.text for doc in documents]\n",
    "        self.embedding_model = embedding_model\n",
    "        self.document_embeddings = self.embed_documents()\n",
    "\n",
    "    def embed_documents(self):\n",
    "        # Encode documents once during initialization.\n",
    "        return self.embedding_model.encode(self.documents)\n",
    "\n",
    "    def retrieve(self, query, top_k=10, similarity_threshold=0.3):\n",
    "        # Encode query and compute cosine similarities.\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        similarities = np.dot(self.document_embeddings, query_embedding.T).flatten()\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        # Filter documents based on similarity threshold.\n",
    "        relevant_docs = [self.documents[i] for i in top_indices if similarities[i] >= similarity_threshold]\n",
    "        return relevant_docs\n",
    "\n",
    "# --- RAG Pipeline ---\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def run(self, query):\n",
    "        # Retrieve relevant documents.\n",
    "        retrieved_docs = self.retriever.retrieve(query)\n",
    "        if not retrieved_docs:\n",
    "            return \"Not available or not in context.\", retrieved_docs\n",
    "        # Combine retrieved documents into a context string.\n",
    "        context = \"\\n\".join(retrieved_docs)\n",
    "        # Generate answer based on query and context.\n",
    "        response = generate_with_deepseek(query, context)\n",
    "        return response, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming eval_dataset is a list of dictionaries with keys: 'query', 'context', 'response'\n",
    "eval_data_df = pd.DataFrame(eval_dataset)  # Convert eval_dataset to a DataFrame\n",
    "\n",
    "# Save the extracted data to CSV\n",
    "csv_save_path = \"evaluation_data.csv\"\n",
    "eval_data_df.to_csv(csv_save_path, index=False)\n",
    "\n",
    "print(f\"Evaluation dataset saved to {csv_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DocumentRetriever(document_chunks, embedding_model)\n",
    "rag_pipeline = RAGPipeline(retriever)\n",
    "\n",
    "#  generate response and context for the first five questions in the eval_data_df\n",
    "eval_df_new = eval_data_df.head(2).copy()\n",
    "# now create a new dataframe with additional columns with the generated response and retrieved context using the RAG pipeline\n",
    "eval_df_new[[\"generated_response\", \"retrieved_context\"]] = eval_df_new.apply(lambda row: rag_pipeline.run(row[\"question\"]), axis=1, result_type=\"expand\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_resp_cont = eval_data_df.copy()\n",
    "\n",
    "# Apply the RAG pipeline to each question and create two new columns:\n",
    "# \"generated_response\" and \"retrieved_context\"\n",
    "eval_df_resp_cont[[\"generated_response\", \"retrieved_context\"]] = eval_df_resp_cont.apply(\n",
    "    lambda row: rag_pipeline.run(row[\"question\"]), axis=1, result_type=\"expand\"\n",
    ")\n",
    "\n",
    "# save the new dataframe to a CSV file\n",
    "eval_df_resp_cont.to_csv(\"eval_data_with_responses.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation data with responses saved to eval_data_with_responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
