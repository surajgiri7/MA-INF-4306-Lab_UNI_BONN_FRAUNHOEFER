{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    " \n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from chromadb import PersistentClient\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load OpenAI API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset splits\n",
    "corpus_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\", split=\"passages\")\n",
    "eval_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Convert the text corpus into Document objects\n",
    "documents = [example['passage'] for example in corpus_dataset]\n",
    "\n",
    "documents = [Document(text=text, metadata={\"source\": f\"doc_{i}\"}) for i, text in enumerate(documents)]\n",
    "\n",
    "# Extract the text corpus\n",
    "print(f\"Extracted {len(documents)} passages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize storage and embedding\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  \n",
    "chroma_collection = chroma_client.get_or_create_collection(name=\"knowledge_base\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize OpenAI Embedding model\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Embed documents in chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=768, chunk_overlap=56)\n",
    "all_embeddings = []\n",
    "\n",
    "def get_batch_embeddings(batch_chunks):\n",
    "    \"\"\"Fetch embeddings for a batch of chunks.\"\"\"\n",
    "    embeddings = embed_model.embed_documents(batch_chunks)  # Correct method to use after update\n",
    "    return embeddings\n",
    "\n",
    "def process_document(doc, progress_bar):\n",
    "    \"\"\"Process a single document and return its embeddings.\"\"\"\n",
    "    chunks = text_splitter.split_text(doc.text)\n",
    "    batch_size = 100  # Process 100 chunks at once for batching\n",
    "    chunk_batches = [chunks[i:i+batch_size] for i in range(0, len(chunks), batch_size)]\n",
    "    \n",
    "    embeddings_for_doc = []\n",
    "    \n",
    "    # Send batches to be processed concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for batch_embeddings in executor.map(get_batch_embeddings, chunk_batches):\n",
    "            embeddings_for_doc.extend(batch_embeddings)\n",
    "    \n",
    "    # Update the progress bar after processing each document\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    return embeddings_for_doc\n",
    "\n",
    "# Iterate over all documents and process them in parallel with a progress bar\n",
    "def embed_documents(documents):\n",
    "    total_documents = len(documents)\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_documents, desc=\"Processing documents\") as progress_bar:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            embeddings = list(executor.map(process_document, documents, [progress_bar] * total_documents))\n",
    "    \n",
    "    all_embeddings = [embedding for sublist in embeddings for embedding in sublist]\n",
    "    return all_embeddings\n",
    "\n",
    "all_embeddings = embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Step: Create FAISS Index\n",
    "embedding_dim = 1536  # Dimension of text-embedding-ada-002\n",
    "\n",
    "# Initialize FAISS index (using L2 distance)\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Convert embeddings to a NumPy array\n",
    "embedding_matrix = np.array(all_embeddings).astype(np.float32)\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Retrieve Relevant Documents\n",
    "\n",
    "def get_top_k_documents(query, top_k=5):\n",
    "    \"\"\"Retrieve top-k most relevant documents using FAISS.\"\"\"\n",
    "    query_embedding = embed_model.embed_query(query)  # Get query embedding\n",
    "    query_vector = np.array(query_embedding).astype(np.float32).reshape(1, -1)\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "    # Fetch the top-k matching documents\n",
    "    top_docs = [documents[i].text for i in indices[0]]\n",
    "    return top_docs\n",
    "\n",
    "# Test retrieval\n",
    "query = \"When did he die?\"\n",
    "retrieved_docs = get_top_k_documents(query)\n",
    "print(f\"Top documents for query: {query}\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\\n{doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Generate Answer with OpenAI (Updated for latest API)\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    \"\"\"Generate an answer using the query and retrieved documents.\"\"\"\n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Create the augmented prompt\n",
    "    prompt = f\"Directly and briefly answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # Use OpenAI's GPT-4 to generate the answer\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    " \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "print(f\"\\nGenerated Answer:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Evaluate and Aggregate Results\n",
    "\n",
    "def evaluate_system(eval_dataset, top_k=5):\n",
    "    \"\"\"Evaluate the system using OpenAI API and the improved evaluation prompt for a subset of questions.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    " \n",
    "    for example in tqdm(eval_dataset, desc=\"Evaluating system\"):\n",
    "        question = example['question']\n",
    "        ground_truth_answer = example['answer']\n",
    "\n",
    "        # Retrieve relevant documents and generate system answer\n",
    "        retrieved_docs = get_top_k_documents(question, top_k=top_k)\n",
    "        retriever_response = generate_answer(question, retrieved_docs)\n",
    "\n",
    "        # Build the improved evaluation prompt\n",
    "        deepval_prompt = f\"\"\"\n",
    "            You are an expert evaluator assessing a Retrieval-Augmented Generation (RAG) system. Your task is to score the system's answer based on the given question and ground truth.\n",
    "\n",
    "            Please assign scores for the following metrics on a scale of 0 to 1. A score of 1 indicates perfect performance, and 0 indicates complete failure. The score should be based on the given instructions.\n",
    "\n",
    "            **Question:** {question}\n",
    "            **Ground Truth Answer:** {ground_truth_answer}\n",
    "            **System Answer:** {retriever_response}\n",
    "\n",
    "            ### Instructions for Scoring:\n",
    "\n",
    "            - **Answer Relevancy (0-1):** How well does the system answer the question? \n",
    "            - A score of 1 means the answer directly addresses the question. \n",
    "            - A score of 0 means the answer is irrelevant or does not address the question at all. \n",
    "            - Scores between 0 and 1 can be given for partial relevance.\n",
    "\n",
    "            - **Faithfulness (0-1):** Is the system answer factually consistent with the information in the retrieved context?\n",
    "            - A score of 1 means the answer is entirely factually correct and consistent with the retrieved content. \n",
    "            - A score of 0 means the answer contains false information.\n",
    "            - Scores between 0 and 1 should reflect degrees of factual correctness.\n",
    "\n",
    "            - **Contextual Precision (0-1):** How specific and concise is the system answer in relation to the ground truth?\n",
    "            - A score of 1 means the system answer is precise and matches the specific details of the ground truth.\n",
    "            - A score of 0 means the system answer is vague or includes incorrect or irrelevant details.\n",
    "\n",
    "            - **Contextual Recall (0-1):** Does the system answer capture the key information from the ground truth?\n",
    "            - A score of 1 means the answer captures all essential details.\n",
    "            - A score of 0 means the answer misses critical information.\n",
    "            - Scores between 0 and 1 reflect partial recall.\n",
    "\n",
    "            Please respond with the scores in the following format:\n",
    "\n",
    "            {{\n",
    "                \"answer_relevancy\": <value>,\n",
    "                \"faithfulness\": <value>,\n",
    "                \"contextual_precision\": <value>,\n",
    "                \"contextual_recall\": <value>\n",
    "            }}\n",
    "            If the system answer is perfectly correct, return a score of 1 for all metrics and if it was completely wrong return a score of 0.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "        # Get evaluation scores from GPT-4o-mini\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": deepval_prompt}]\n",
    "        )\n",
    "\n",
    "        # Parse the response\n",
    "        eval_scores_raw = response.choices[0].message.content.strip()\n",
    " \n",
    "        try:\n",
    "            eval_scores = json.loads(eval_scores_raw)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse scores for question: {question}\")\n",
    "            eval_scores = {\n",
    "                \"answer_relevancy\": 0,\n",
    "                \"faithfulness\": 0,\n",
    "                \"contextual_precision\": 0,\n",
    "                \"contextual_recall\": 0\n",
    "            }\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth_answer\": ground_truth_answer,\n",
    "            \"retriever_response\": retriever_response,\n",
    "            \"scores\": eval_scores\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Calculate average scores\n",
    "def calculate_average_scores(results):\n",
    "    \"\"\"Calculate the average scores for each metric.\"\"\"\n",
    "    total_scores = {\n",
    "        \"answer_relevancy\": 0,\n",
    "        \"faithfulness\": 0,\n",
    "        \"contextual_precision\": 0,\n",
    "        \"contextual_recall\": 0\n",
    "    }\n",
    "\n",
    "    num_questions = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        scores = result[\"scores\"]\n",
    "        for metric in total_scores:\n",
    "            total_scores[metric] += scores.get(metric, 0)\n",
    "\n",
    "    # Compute averages\n",
    "    avg_scores = {metric: score / num_questions for metric, score in total_scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Run the evaluation for 5 questions  \n",
    "evaluation_results = evaluate_system(eval_dataset)\n",
    "\n",
    "\n",
    "# Print average scores\n",
    "average_scores = calculate_average_scores(evaluation_results)\n",
    "print(\"\\nAverage Scores for 5 Questions:\")\n",
    "for metric, avg_score in average_scores.items():\n",
    "    print(f\"{metric}: {avg_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
